---
title: "Readme"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Data Sources
I'm buying a hard drive to backup my data at home, and I want to buy a drive that's not going to fail.  Fortunately, [BackBlaze has shared all of their data on hard drives and drive failures.](https://www.backblaze.com/b2/hard-drive-test-data.html#downloading-the-raw-hard-drive-test-data) 

Backblaze [did their own analysis](https://www.backblaze.com/blog/backblaze-hard-drive-stats-q1-2020/) of drive failures, but I don't like their approach for 2 reasons:    
1. Their "annualized failure rate" (`Drive Failures / (Drive Days / 365)`) assumes that failure rates are constant over time.  E.g. this assumption means that observing 1 drive for 100 days gives you the exact same information as observing 100 drives for 1 day.    
2. They don't really explain how they derived confidence intervals or used them in their analysis, and pretty much rely on the "annualized failure rate" to make conclusions.  I want to use a confidence interval for my decision making. For a lot more detail on why a confidence interval is a good idea, read Evan Miller's blog post about a different type of problem: [How Not To Sort By Average Rating](https://www.evanmiller.org/how-not-to-sort-by-average-rating.html).    


I wanted to use a failure model that allows for time-varying failure rates, and then pick a drive based on a confidence interval, so here we are.

# Survival Analysis

I wanted to pick my drive based on: `lower 95% confidence interval for median time to failure`.  In other words, I want to pick the drive model that has the most evidence it will last a large number of days.

In order to analyze median time to failure, you need to observe your sample long enough for 50% of the drives to fail.  However, these drives are **so reliable** that almost none of the models in the sample have yet hit the 50% failure mark.  Therefore, I will settle for looking at `upper 95% confidence interval for failure rate after 1 year`.  In other words, I want to pick the drive I am most sure will last at least one year.

Some technical notes:    
1. I only looked at drive models where at least 100 individual drives lasted a year or longer, to remove drive models without a lot of data.  (I don't love this, and wish I knew how to make the survival curve confidence intervals reflect uncertainty from the number of individuals observed).    
2. This analysis does not assume a constant failure rate for each drive model.  We often see in real life that drives fail at a high rate early on, and then failures become less likely over time.    
3. This analysis allows different drive models to have different failure "curves."  I looped over every drive model, ran the [survfit](https://www.rdocumentation.org/packages/survival/versions/2.11-4/topics/survfit) function in R (which fits a very simple, non-parametric [Kaplan-Meier survival curve](https://en.wikipedia.org/wiki/Kaplan%E2%80%93Meier_estimator)), and then took the 95% confidence interval at 1 year from the fitted survival curve.    

```{r surv, echo=FALSE}
# Setup
library(data.table)
library(survival)
days_to_year <- 365.2425

data_raw <- fread('all_data.csv')
keys <- c('model', 'serial_number')
setkeyv(data_raw, keys)
data_raw <- data_raw[,list(
  drive_days = sum(N),
  failure=sum(failure),
  capacity_tb=round(max(capacity_bytes)/1e+12, 1)
), by=keys]

# Only keep models were at leat 100 drives made it to one year
data_raw[,count_one_year := sum(drive_days>=days_to_year), by='model']
data_raw <- data_raw[count_one_year > 99,]

# Do a non-parametric survival curve for every drive model
survival_curve_at_t <- function(time, failure, at=days_to_year){
  out <- survfit(Surv(time, failure)~1)
  out <- summary(out, times=at, conf.int=.95)
  out <- list(
    surv = out$surv,
    lower =  out$lower
  )
  return(out)
}

data_surv <- data_raw[, c(list(
  capacity_tb=max(capacity_tb), 
  drive_days=sum(drive_days),
  failures=sum(failure),
  N_drives=.N
  ), survival_curve_at_t(drive_days, failure)), by='model']
data_surv <- data_surv[order(-lower),]

# Choose best drive
best_drive <- data_surv[1, model]
```

Here's the results of our analysis.  The `r best_drive` is the most reliable drive model in our sample of data:
```{r surv_results, echo=FALSE}
knitr::kable(
  data_surv[,
    list(
      model, 
      capacity_tb, 
      drive_days,
      failures,
      one_year_failure_rate=sprintf("%1.2f%%", 100*(1-surv)),
      ci_95=sprintf("%1.2f%%", 100*(1-lower)))
    ]
)
```

# 12TB vs 4TB drives
```{r choose_12_v_4, echo=FALSE}
best_drive_12 <- data_surv[round(capacity_tb)==12,][1, model]
best_drive_4 <- data_surv[round(capacity_tb)==4,][1, model]
```

Lets drill down into my results a little bit, and compare our best 12TB drive (`r best_drive_12`) to the best 4TB drive (`r best_drive_4`):

```{r table_12_v_4, echo=FALSE}
knitr::kable(
  data_surv[model %in% c(best_drive_12, best_drive_4),][,
    list(
      model, 
      capacity_tb, 
      drive_days,
      failures,
      N_drives,
      one_year_failure_rate=sprintf("%1.2f%%", 100*(1-surv)),
      ci_95=sprintf("%1.2f%%", 100*(1-lower)))
    ]
)
```

Let's use Backblaze's "naive" statistic to compare these 2 drives: `Drive Failures / (Drive Days / 365)`

```{r table_12_v_4_drilldown, echo=FALSE}
result <- data_surv[model %in% c(best_drive_12, best_drive_4),][,
    list(
      model, 
      capacity_tb, 
      drive_days,
      one_year_failure_rate=sprintf("%1.2f%%", 100*(1-surv)),
      naive_rate=sprintf("%1.2f%%", 100*(failures/(drive_days/365)))
    )
  ]
knitr::kable(result)
rate_12 <- result[round(capacity_tb)==12,][1, naive_rate]
rate_4 <- result[round(capacity_tb)==4,][1, naive_rate]
```

The `r best_drive_12` actually has a **higher* naive failure rate at `r rate_12` than the `r best_drive_4` at `r rate_4`.

So why do I reccomend buying `r best_drive_4`?

The answer is that we've observed the `r best_drive_4` for many more years than the `r best_drive_12`.  On the one had, these extra years give us more certainty that these drives are reliable and fail at extremely low rates.  On the other hand, these extra years are **late** in the drive's lifetimes, when failure rates are (expected) to be lower.

Zooming in on the first year of a drive's life demonstates that the `r rate_12` has a lower failure rate during this period.  Obviously the future beyond that is unknown, but I expect this lower failure rate to continue over the lifetime of the two drives:

```{r plot_2_drives, echo=FALSE, plot=TRUE}
# Plot survival curves for 1 or 2 models to check results make sense
library(survminer)
dat <- data_raw[model %in% c(best_drive_12, best_drive_4),]
model <- dat[,survfit(Surv(drive_days, failure) ~ model)]
#ggsurvplot(model, data=dat, fun='cumhaz', conf.int = T)
ggsurvplot(model, data=dat, conf.int = T, ylim=c(.95, 1.0))
```

# Replicating my results
[all_data.csv](all_data.csv) has the cleaned up data from backblaze, at the level of individual drives, days observed, and whether or not the drive failed.   

[README.Rmd](README.Rmd) has the code to run this analysis and generate this [README.md](README.md) file you are reading right now. Use [RStudio](https://rstudio.com/products/rstudio/download/) to `knit` the `Rmd` file into a `md` file, which github will then render nicely for you. [knitr::kable](https://www.rdocumentation.org/packages/knitr/versions/1.29/topics/kable) produces the nice table of results.

If you want to get the raw data before it was cleaned up into [all_data.csv](all_data.csv), you'll need at least 70GB of free hard drive space.  I also suggest opening [backblaze_analysis.Rproj](backblaze_analysis.Rproj) in RStudio.    
1.  Run [1_download_data.R](1_download_data.R) to download the data (almost 10.5 GB).    
2.  Run [2_unzip_data.R](2_unzip_data.R) to unzip the data (almost 55 GB).    
3.  Run [3_assemble_data.R](3_assemble_data.R) to "compress" the data, which generates [all_data.csv](all_data.csv).    

An interesting note about this data: It's 55GB uncompressed, and contains a whole bunch of irrelevant informtation.  It was very interesting to me that I could compress a 55GB dataset to 10mb, while still keeping **all** of the relevant information for modeling.  (In other words, this dataset was 5,000x larger than it needed to be). I think this is another example of how "good data structures" are essential for effective engineering, and data science is, at its core, engineering.

# Erratum
![I nerd sniped myself](https://imgs.xkcd.com/comics/nerd_sniping.png)
