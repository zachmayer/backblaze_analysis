---
title: "Readme"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Data Sources
I'm buying a hard drive to backup my data at home, and I want to buy a drive that's not going to fail.  Fortunately, [BackBlaze has shared all of their data on hard drives and drive failures.](https://www.backblaze.com/b2/hard-drive-test-data.html#downloading-the-raw-hard-drive-test-data) 

Backblaze [did their own analysis](https://www.backblaze.com/blog/backblaze-hard-drive-stats-q1-2020/) of drive failures, but I don't like their approach for 2 reasons:    
1. Their "annualized failure rate" (`Drive Failures / (Drive Days / 365)`) assumes that failure rates are constant over time.  E.g. this assumption means that observing 1 drive for 100 days gives you the exact same information as observing 100 drives for 1 day.    
2. They don't really explain how they derived confidence intervals or used them in their analysis, and pretty much rely on the "annualized failure rate" to make conclusions.  I want to use a confidence interval for my decision making. For a lot more detail on why a confidence interval is a good idea, read Evan Miller's blog post about a different type of problem: [How Not To Sort By Average Rating](https://www.evanmiller.org/how-not-to-sort-by-average-rating.html).    


I wanted to use a failure model that allows for time-varying failure rates, and then pick a drive based on a confidence interval, so here we are.

# Survival Analysis

I wanted to pick my drive based on: `lower 95% confidence interval for median time to failure`.  In other words, I want to pick the drive model that has the most evidence it will last a large number of days.

In order to analyze median time to failure, you need to observe your sample long enough for 50% of the drives to fail.  However, these drives are **so reliable** that almost none of the models in the sample have yet hit the 50% failure mark.  Therefore, I will settle for looking at `upper 95% confidence interval for failure rate after 1 year`.  In other words, I want to pick the drive I am most sure will last at least one year.

Some technical notes:    
1. I only looked at drive models where at least 100 individual drives lasted a year or longer, to remove drive models without a lot of data.  (I don't love this, and wish I knew how to make the survival curve confidence intervals reflect uncertainty from the number of individuals observed).    
2. This analysis does not assume a constant failure rate for each drive model.  We often see in real life that drives fail at a high rate early on, and then failures become less likely over time.    
3. This analysis allows different drive models to have different failure "curves."  I looped over every drive model, ran the [survfit](https://www.rdocumentation.org/packages/survival/versions/2.11-4/topics/survfit) function in R (which fits a very simple, non-parametric [Kaplan-Meier survival curve](https://en.wikipedia.org/wiki/Kaplan%E2%80%93Meier_estimator)), and then took the 95% confidence interval at 1 year from the fitted survival curve.    

```{r surv, echo=FALSE}
# Setup
library(data.table)
library(survival)
days_to_year <- 365.2425

data_raw <- fread('all_data.csv')
keys <- c('model', 'serial_number')
setkeyv(data_raw, keys)
data_raw <- data_raw[,list(
  drive_days = sum(N),
  failure=sum(failure),
  capacity_tb=round(max(capacity_bytes)/1e+12, 1)
), by=keys]

# Only keep models were at leat 100 drives made it to one year
data_raw[,count_one_year := sum(drive_days>=days_to_year), by='model']
data_raw <- data_raw[count_one_year > 99,]

# Do a non-parametric survival curve for every drive model
survival_curve_at_t <- function(time, failure, at=days_to_year){
  out <- survfit(Surv(time, failure)~1)
  out <- summary(out, times=at, conf.int=.95)
  out <- list(
    surv = out$surv,
    lower =  out$lower
  )
  return(out)
}

data_surv <- data_raw[, c(list(
  capacity_tb=max(capacity_tb), 
  drive_days=sum(drive_days),
  failures=sum(failure),
  N_drives=.N
  ), survival_curve_at_t(drive_days, failure)), by='model']
data_surv <- data_surv[order(-lower),]

# Choose best drive
best_drive <- data_surv[1, model]
```

Here's the results of our analysis.  The `r best_drive` is the most reliable drive model in our sample of data:
```{r surv_results, echo=FALSE}
knitr::kable(
  data_surv[,
    list(
      model, 
      capacity_tb, 
      drive_days,
      failures,
      one_year_failure_rate=sprintf("%1.2f%%", 100*(1-surv)),
      ci_95=sprintf("%1.2f%%", 100*(1-lower)))
    ]
)
```

# 12TB vs 4TB drives
```{r choose_12_v_4, echo=FALSE}
best_drive_12 <- data_surv[round(capacity_tb)==12,][1, model]
best_drive_4 <- data_surv[round(capacity_tb)==4,][1, model]
```

Lets drill down into my results a little bit, and compare our best 12TB drive (`r best_drive_12`) to the best 4TB drive (`r best_drive_4`):

```{r table_12_v_4, echo=FALSE}
knitr::kable(
  data_surv[model %in% c(best_drive_12, best_drive_4),][,
    list(
      model, 
      capacity_tb, 
      drive_days,
      failures,
      N_drives,
      one_year_failure_rate=sprintf("%1.2f%%", 100*(1-surv)),
      ci_95=sprintf("%1.2f%%", 100*(1-lower)))
    ]
)
```

Let's use Backblaze's "naive" statistic to compare these 2 drives: `Drive Failures / (Drive Days / 365)`

```{r table_12_v_4_drilldown, echo=FALSE}
result <- data_surv[model %in% c(best_drive_12, best_drive_4),][,
    list(
      model, 
      capacity_tb, 
      drive_days,
      one_year_failure_rate=sprintf("%1.2f%%", 100*(1-surv)),
      naive_rate=sprintf("%1.2f%%", 100*(failures/(drive_days/365)))
    )
  ]
knitr::kable(result)
rate_12 <- result[round(capacity_tb)==12,][1, naive_rate]
rate_4 <- result[round(capacity_tb)==4,][1, naive_rate]
```

The `r best_drive_12` actually has a **higher** naive failure rate at `r rate_12` than the `r best_drive_4` at `r rate_4`.

So why do I reccomend buying `r best_drive_4`?

The answer is that we've observed the `r best_drive_4` for many more years than the `r best_drive_12`.  On the one had, these extra years give us more certainty that these drives are reliable and fail at extremely low rates.  On the other hand, these extra years are **late** in the drive's lifetimes, when failure rates are (expected) to be lower.

Zooming in on the first year of a drive's life demonstates that the `r rate_12` has a lower failure rate during this period.  Obviously the future beyond that is unknown, but I expect this lower failure rate to continue over the lifetime of the two drives:

```{r plot_2_drives, echo=FALSE}
# Plot survival curves for 1 or 2 models to check results make sense
suppressWarnings(suppressMessages(library(survminer)))
dat <- data_raw[model %in% c(best_drive_12, best_drive_4),]
model <- dat[,survfit(Surv(drive_days, failure) ~ model)]
#ggsurvplot(model, data=dat, fun='cumhaz', conf.int = T)
out <- ggsurvplot(model, data=dat, conf.int = T, ylim=c(.96, 1.0), conf.int.style='step')
print(out)
```

# Replicating my results
[all_data.csv](all_data.csv) has the cleaned up data from backblaze, at the level of individual drives, days observed, and whether or not the drive failed.   

[README.Rmd](README.Rmd) has the code to run this analysis and generate this [README.md](README.md) file you are reading right now. Use [RStudio](https://rstudio.com/products/rstudio/download/) to `knit` the `Rmd` file into a `md` file, which github will then render nicely for you. [knitr::kable](https://www.rdocumentation.org/packages/knitr/versions/1.29/topics/kable) produces the nice table of results.

If you want to get the raw data before it was cleaned up into [all_data.csv](all_data.csv), you'll need at least 70GB of free hard drive space.  I also suggest opening [backblaze_analysis.Rproj](backblaze_analysis.Rproj) in RStudio.    
1.  Run [1_download_data.R](1_download_data.R) to download the data (almost 10.5 GB).    
2.  Run [2_unzip_data.R](2_unzip_data.R) to unzip the data (almost 55 GB).    
3.  Run [3_assemble_data.R](3_assemble_data.R) to "compress" the data, which generates [all_data.csv](all_data.csv).    

An interesting note about this data: It's 55GB uncompressed, and contains a whole bunch of irrelevant informtation.  It was very interesting to me that I could compress a 55GB dataset to 10mb, while still keeping **all** of the relevant information for modeling.  (In other words, this dataset was 5,000x larger than it needed to be). I think this is another example of how "good data structures" are essential for effective engineering, and data science is, at its core, engineering.

# Some notes on survival analysis
Survival analysis is a little weird, becuase you don't observe teh full distribution of your data.  This makes some traditional statistics impossible to calculate.

For example, until you observe every hard drive in the sample fail, you can't know the mean failure rate.  (For example if you have one unit left that hasn't failed yet, and becomes an outlier in survival time, that might have a big impact on mean survivial time.)

Therefore, for most survival problems, we use quantiles.  For example, once you've observed 50% of the hard drives fail, you can compute median time to failure.  Or perhaps once you've observed 10% of the drives fail, you can calculate the 10th percentile of time to failure.

Here's the thing: these drivew are **so reliable** even after 5 years of data, we've barely observed the distribution of failures.  (This is a good thing, but it makes it hard to chose between drives!).

Here's our best 12TB and 4TB drives:
```{r percentiles_for_best_drive, echo=FALSE}
best_drives <- data_surv[model %in% c(best_drive_12, best_drive_4),][,
    list(
      model, 
      capacity_tb, 
      failures,
      N_drives,
      pct_failed = sprintf("%1.2f%%", 100*(failures/N_drives))
    )
  ]
failure_rate <- data_surv[model %in% c(best_drive_12, best_drive_4), max(100*(failures/N_drives))]
failure_rate <- sprintf("%1.2f%%", failure_rate)
knitr::kable(best_drives)
```

So at most, we've seen `r failure_rate` drives fail, which means we're at most seeing the `r failure_rate` percentile for drive survival time— we cannot make any inferences about percentiles higher than this, becuase **not enough drives have failed yet!**.

# Erratum
![I nerd sniped myself](https://imgs.xkcd.com/comics/nerd_sniping.png)
